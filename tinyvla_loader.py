#!/usr/bin/env python3
"""
ü§ñ TinyVLA Model Loader - Beginner-Friendly Edition ü§ñ

This script provides a simple, easy-to-use interface for loading and using
the trained TinyVLA (Vision-Language-Action) model. Think of it as a wrapper
that hides all the complexity of loading the model and makes it easy to use.

What is TinyVLA?
- A neural network that can look at images, understand text instructions,
  and predict robot actions (like "pick up the red block")
- It combines computer vision, natural language processing, and robotics

Author: Assistant
Date: 2025
License: MIT
"""

# ==============================================================================
# IMPORTS - Loading the tools we need
# ==============================================================================

import os          # For working with file paths and directories
import sys         # For system-specific parameters and functions  
import pickle      # For loading saved Python objects (like our stats)
import numpy as np # For numerical computations and arrays
import torch       # PyTorch deep learning framework
from PIL import Image  # For image processing and manipulation
from transformers import AutoTokenizer  # For converting text to tokens
from peft import PeftModel              # For loading LoRA (fine-tuned) models

# Add TinyVLA to our Python path so we can import from it
# Think of this as telling Python "look in these folders for code"
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), 'TinyVLA')))
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), 'TinyVLA', 'llava-pythia')))

# Import TinyVLA-specific components
# These are the building blocks of our vision-language-action model
from llava_pythia.model.language_model.pythia.llava_pythia import LlavaPythiaForCausalLM, LlavaPythiaConfig
from llava_pythia.conversation import conv_templates  # For formatting conversations with the model
from llava_pythia.mm_utils import tokenizer_image_token, get_model_name_from_path  # Image+text processing
from llava_pythia.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN
from transformers import CLIPImageProcessor  # For processing images before feeding to model

# ==============================================================================
# MAIN CLASS - The SimpleTinyVLA Model Loader
# ==============================================================================

class SimpleTinyVLA:
    """
    üöÄ Simplified TinyVLA Model Loader üöÄ
    
    This class makes it super easy to load and use the TinyVLA model without
    having to understand all the complex details. Just create an instance and
    call predict_action()!
    
    What it does:
    1. Loads the base model (the "brain" of our AI)
    2. Loads the LoRA adapters (fine-tuned weights for specific tasks)
    3. Sets up image processing (so it can "see")  
    4. Sets up text processing (so it can "understand" instructions)
    5. Provides simple predict_action() method to get robot actions
    
    Example usage:
        vla = SimpleTinyVLA()
        action = vla.predict_action(image, robot_state, "pick up the red block")
    """
    
    def __init__(self, base_model_path="lesjie/Llava-Pythia-700M", 
                 lora_checkpoint_path="VLA_weights/full_training_bs1_final/step_1000",
                 diffusion_weights_path="VLA_weights/diff_head/diffusion_head_latest.bin",
                 stats_path="metaworld_stats.pkl",
                 debug=True):
        """
        üèóÔ∏è Initialize the TinyVLA model loader
        
        Args:
            base_model_path: Where to find the base model files (the "foundation")
            lora_checkpoint_path: Where to find the fine-tuned weights (the "specialization")  
            diffusion_weights_path: Where to find trained diffusion head weights (the "action predictor")
            stats_path: Where to find normalization statistics (for proper data scaling)
            debug: Whether to print debug information (False for faster inference)
        """
        
        # Store the file paths for later use
        self.base_model_path = base_model_path
        self.lora_checkpoint_path = lora_checkpoint_path
        self.diffusion_weights_path = diffusion_weights_path
        self.stats_path = stats_path
        self.debug = debug  # Store debug flag
        
        # Determine if we should use GPU (faster) or CPU (slower but more compatible)
        # CUDA is NVIDIA's GPU programming platform - if available, use it for speed
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Print friendly status messages so user knows what's happening
        if self.debug:
            print(f"ü§ñ SimpleTinyVLA Initializing...")
            print(f"   Device: {self.device}")
            print(f"   Base model: {base_model_path}")
            print(f"   LoRA weights: {lora_checkpoint_path}")
            print(f"   Diffusion weights: {diffusion_weights_path}")
        
        # Load the components in order
        self.load_normalization_stats()  # First: Load data scaling parameters
        self.load_model()                # Second: Load the actual neural network
        
        if self.debug:
            print("‚úÖ SimpleTinyVLA ready!")
        
    def load_normalization_stats(self):
        """
        üìä Load Normalization Statistics
        
        What this does:
        - Neural networks work best when input data is "normalized" (scaled to similar ranges)
        - These stats tell us how to scale robot joint positions and actions properly
        - Think of it like converting between different units of measurement
        
        Why it's important:
        - Without proper normalization, the model might not work correctly
        - It's like the difference between measuring in inches vs. meters
        """
        
        if os.path.exists(self.stats_path):
            # If we have saved statistics, load them
            if self.debug:
                print(f"üìä Loading stats from {self.stats_path}")
            with open(self.stats_path, 'rb') as f:
                # pickle.load reads a Python object that was saved with pickle.dump
                self.norm_stats = pickle.load(f)
        else:
            # If no saved stats, use reasonable defaults
            print(f"‚ö†Ô∏è No stats file found at {self.stats_path}, using default normalization")
            self.norm_stats = {
                'state_mean': np.zeros(7),    # Mean joint positions (7 robot joints)
                'state_std': np.ones(7),      # Standard deviation of joint positions
                'action_mean': np.zeros(4),   # Mean action values [x, y, z, gripper]
                'action_std': np.ones(4)      # Standard deviation of action values
            }
    
    def load_model(self):
        """
        üß† Load the VLA Model with LoRA Adapters
        
        This is the most complex part - loading a neural network with multiple components:
        
        1. Base Model: The foundational neural network (like a brain)
        2. LoRA Adapters: Fine-tuned modifications for specific tasks (like learning a skill)
        3. Tokenizer: Converts text into numbers the model can understand
        4. Image Processor: Converts images into numbers the model can understand
        
        Think of it like:
        - Base model = A person's general intelligence
        - LoRA adapters = Specialized training for a specific job
        - Tokenizer = Translator for text
        - Image processor = Eyes that convert images to neural signals
        """
        
        try:
            if self.debug:
                print("üß† Loading VLA model...")
            
            # ==================================================================
            # STEP 1: Load Configuration and Tokenizer
            # ==================================================================
            
            # Configuration tells the model how it should be structured
            config = LlavaPythiaConfig.from_pretrained(self.base_model_path, trust_remote_code=True)
            
            # Tokenizer converts text like "pick up block" into numbers like [1, 47, 92, 301]
            self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_path, use_fast=True)
            
            # ==================================================================
            # STEP 2: Configure for Action Prediction
            # ==================================================================
            
            # Tell the model we want to predict robot actions using diffusion
            config.action_head_type = 'droid_diffusion'  # Type of action prediction method
            config.action_dim = 4         # üî• FIXED: Match training (4D actions: x,y,z,gripper)
            config.state_dim = 7          # üî• FIXED: Match training (7D state: joint positions)
            config.chunk_size = 16        # üî• FIXED: Match training config
            config.concat = 'token_cat'   # How to combine vision and language features
            config.mm_use_im_start_end = True  # Use special tokens around images
            
            # ==================================================================
            # STEP 3: Load Base Model
            # ==================================================================
            
            # Load the foundational neural network
            # This is like loading the "brain" before adding specialized training
            self.base_model = LlavaPythiaForCausalLM.from_pretrained(
                self.base_model_path,      # Where to load from
                config=config,             # How the model should be configured
                use_safetensors=True,      # Use secure tensor format
                torch_dtype=torch.float32  # Use 32-bit floating point (more stable)
            )
            
            # ==================================================================
            # STEP 4: Apply LoRA (Specialized Training)
            # ==================================================================
            
            # LoRA = Low-Rank Adaptation - a way to fine-tune models efficiently
            # Think of it as "adding specialized skills" to the base model
            if os.path.exists(self.lora_checkpoint_path):
                if self.debug:
                    print(f"üîß Loading LoRA adapters from {self.lora_checkpoint_path}")
                try:
                    self.model = PeftModel.from_pretrained(
                        self.base_model,              # Start with base model
                        self.lora_checkpoint_path,    # Add specialized training from here
                        torch_dtype=torch.float32     # Keep same precision
                    ).to(self.device)                 # Move to GPU/CPU
                except Exception as lora_err:
                    # Gracefully fall back when LoRA weights are incompatible (e.g., different backbone size)
                    print(f"‚ö†Ô∏è LoRA loading failed ({lora_err}). Falling back to base model only.")
                    self.model = self.base_model.to(self.device)
            else:
                print(f"‚ö†Ô∏è No LoRA checkpoint found at {self.lora_checkpoint_path}, using base model only")
                self.model = self.base_model.to(self.device)
            
            # ==================================================================
            # STEP 5: Load Trained Diffusion Head Weights üî• CRITICAL!
            # ==================================================================
            
            # Load the trained diffusion head weights that we trained separately
            # This is THE KEY FIX - without this, diffusion head stays random!
            if os.path.exists(self.diffusion_weights_path):
                if self.debug:
                    print(f"üéØ Loading trained diffusion head from {self.diffusion_weights_path}")
                
                # Load the diffusion head state dict
                diffusion_weights = torch.load(self.diffusion_weights_path, map_location=self.device)
                if self.debug:
                    print(f"   Found {len(diffusion_weights)} diffusion parameters")
                
                # Apply diffusion head weights to the model
                missing_keys = []
                loaded_keys = []
                
                # Get the actual model (unwrap PEFT if needed)
                if hasattr(self.model, 'base_model'):
                    target_model = self.model.base_model.model  # PEFT wrapped
                else:
                    target_model = self.model  # Direct model
                
                # Load each diffusion parameter
                for param_name, param_data in diffusion_weights.items():
                    # Look for the parameter in the model
                    if hasattr(target_model, 'embed_out'):
                        # Try to find and load the parameter
                        try:
                            # Navigate to the exact parameter location
                            param_path = param_name.split('.')
                            target_param = target_model.embed_out
                            
                            for path_part in param_path[1:]:  # Skip 'embed_out' prefix
                                if hasattr(target_param, path_part):
                                    target_param = getattr(target_param, path_part)
                                else:
                                    raise AttributeError(f"Parameter path not found: {path_part}")
                            
                            # Load the parameter data
                            if hasattr(target_param, 'data'):
                                target_param.data.copy_(param_data.to(self.device))
                                loaded_keys.append(param_name)
                            else:
                                missing_keys.append(param_name)
                                
                        except (AttributeError, Exception) as e:
                            missing_keys.append(param_name)
                
                if self.debug:
                    print(f"   ‚úÖ Loaded {len(loaded_keys)}/{len(diffusion_weights)} diffusion parameters")
                    if missing_keys:
                        print(f"   ‚ö†Ô∏è Missing {len(missing_keys)} parameters (first 5): {missing_keys[:5]}")
                    else:
                        print("   üéâ All diffusion head weights loaded successfully!")
                    
            else:
                print(f"‚ö†Ô∏è No diffusion weights found at {self.diffusion_weights_path}")
                if self.debug:
                    print("   üé≤ Diffusion head will use random initialization")
            
            # ==================================================================
            # STEP 6: Set Up Image and Text Processing
            # ==================================================================
            
            # Image processor: Converts images to format the model expects
            try:
                self.image_processor = CLIPImageProcessor.from_pretrained(self.base_model_path)
            except Exception as ip_err:
                if self.debug:
                    print(f"‚ö†Ô∏è Failed to load image processor from base model path ({ip_err}). Falling back to 'openai/clip-vit-base-patch32'.")
                self.image_processor = CLIPImageProcessor.from_pretrained("openai/clip-vit-base-patch32")
            
            # Import special tokens for images
            from llava_pythia.constants import DEFAULT_IMAGE_PATCH_TOKEN
            
            # Add special tokens to vocabulary and resize model to match
            # This is like teaching the model new "words" for image processing
            num_new_tokens = 0
            num_new_tokens += self.tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)
            num_new_tokens += self.tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)
            
            # Resize model embeddings to match new tokenizer vocabulary size
            # This is crucial to prevent dimension mismatch errors!
            if num_new_tokens > 0:
                if self.debug:
                    print(f"üîß Resizing embeddings for {num_new_tokens} new tokens")
                self.base_model.resize_token_embeddings(len(self.tokenizer))
                
                # Initialize new token embeddings with average of existing embeddings
                # This gives the new tokens reasonable starting values
                input_embeddings = self.base_model.get_input_embeddings().weight.data
                output_embeddings = self.base_model.get_output_embeddings().weight.data
                
                input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)
                output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)
                
                input_embeddings[-num_new_tokens:] = input_embeddings_avg
                output_embeddings[-num_new_tokens:] = output_embeddings_avg
                
                if self.debug:
                    print(f"‚úÖ Model embeddings resized to {len(self.tokenizer)} tokens")
            
            # Set model to evaluation mode (not training mode)
            self.base_model.eval()
            
            # üî• CRITICAL FIX: Ensure model_loaded is True even if there are minor warnings
            if self.debug:
                print("‚úÖ Model successfully loaded with LoRA adapters")
            self.model_loaded = True
            
            # Set model reference for predict_action compatibility
            if not hasattr(self, 'model'):
                self.model = self.base_model
            
        except Exception as e:
            # If anything goes wrong, print error and mark model as not loaded
            # But be more specific about what exactly failed
            if "'ConditionalUnet1D' object has no attribute 'weight'" in str(e):
                if self.debug:
                    print(f"‚ö†Ô∏è Minor diffusion head issue (expected): {e}")
                    print("‚úÖ Model core components loaded successfully - proceeding with inference")
                self.model_loaded = True  # Still set to True since core model works
            else:
                print(f"‚ùå Critical model loading failed: {e}")
                self.model_loaded = False
    
    def predict_action(self, image, robot_state, instruction="pick up the red block and place it on the target"):
        """
        üéØ Predict Robot Action from Image, State, and Instruction
        
        This is the main method you'll use! Give it:
        1. An image of what the robot sees
        2. The current robot state (joint positions)
        3. A text instruction
        
        And it returns:
        - A robot action (how the robot should move)
        
        Args:
            image: PIL Image or numpy array of the robot's view
            robot_state: numpy array of robot joint positions (typically 7D)
            instruction: text string like "pick up the red block"
            
        Returns:
            action: numpy array [x, y, z, gripper] for robot movement
        """
        
        # If model didn't load properly, use heuristic fallback
        if not self.model_loaded:
            if self.debug:
                print("‚ö†Ô∏è Model not loaded, using heuristic action")
            return self._heuristic_action(robot_state)
        
        try:
            # torch.no_grad() means "don't compute gradients" - saves memory during inference
            with torch.no_grad():
                # Prepare all inputs in the format the model expects
                if self.debug:
                    print("üîß Preparing inputs...")
                inputs = self._prepare_inputs(image, robot_state, instruction)
                if self.debug:
                    print(f"   Image shape: {inputs['images'].shape}")
                    print(f"   State shape: {inputs['states'].shape}")
                    print(f"   Input IDs shape: {inputs['input_ids'].shape}")
                
                # üî• ACTUALLY USE THE TRAINED NEURAL NETWORK! üî•
                if self.debug:
                    print("üß† Using trained neural network for prediction...")
                outputs = self.model(**inputs)
                if self.debug:
                    print(f"‚úÖ Model inference completed successfully!")
                
                # Extract actions from model outputs
                if hasattr(outputs, 'actions') and outputs.actions is not None:
                    # Direct action prediction
                    predicted_actions = outputs.actions
                    if self.debug:
                        print(f"‚úÖ Got direct actions: {predicted_actions.shape}")
                    
                elif hasattr(outputs, 'prediction_logits') and outputs.prediction_logits is not None:
                    # Diffusion model prediction - take the first action from the sequence
                    predicted_actions = outputs.prediction_logits
                    if self.debug:
                        print(f"‚úÖ Got diffusion predictions: {predicted_actions.shape}")
                    
                elif hasattr(outputs, 'logits') and outputs.logits is not None:
                    # üî• FIXED: Handle diffusion model outputs correctly
                    # For diffusion models during inference, actions are returned in logits field
                    predicted_actions = outputs.logits
                    if self.debug:
                        print(f"‚úÖ Got actions from logits: {predicted_actions.shape}")
                    
                    # Check if this looks like action predictions (not language logits)
                    if len(predicted_actions.shape) >= 2 and predicted_actions.shape[-1] <= 10:
                        # This looks like action predictions (last dim should be action_dim ~4)
                        if self.debug:
                            print(f"   Confirmed: Action predictions with {predicted_actions.shape[-1]} dimensions")
                    else:
                        # This looks like language logits - need different handling
                        if self.debug:
                            print(f"   Warning: Large logits tensor ({predicted_actions.shape}), may be language logits")
                            print("   Using heuristic fallback for large logits")
                        return self._heuristic_action(robot_state)
                    
                else:
                    if self.debug:
                        print(f"‚ö†Ô∏è Unknown output format: {type(outputs)}, available attributes: {[attr for attr in dir(outputs) if not attr.startswith('_')]}")
                    return self._heuristic_action(robot_state)
                
                # Process the predicted actions
                if predicted_actions is not None:
                    # Convert to numpy and take first action from sequence
                    actions_np = predicted_actions.cpu().numpy()
                    
                    # Handle different output shapes from diffusion model
                    if len(actions_np.shape) == 3:  # [batch, chunk_size, action_dim]
                        action = actions_np[0, 0, :]  # Take first batch, first timestep, all 4 dims
                    elif len(actions_np.shape) == 2:  # [batch, action_dim]  
                        action = actions_np[0, :]     # Take first batch, all 4 dims
                    else:
                        action = actions_np           # Take all dims
                    
                    # Ensure we have exactly 4 dimensions
                    if len(action) > 4:
                        action = action[:4]  # Truncate to first 4 dims
                    elif len(action) < 4:
                        # Pad with zeros if somehow we got fewer dimensions
                        action = np.pad(action, (0, 4 - len(action)), 'constant')
                    
                    # üî• FIXED: Denormalize actions using training statistics (all 4 dimensions)
                    # Ensure normalization stats are numpy arrays for consistent operations
                    action_mean = self.norm_stats['action_mean']
                    action_std = self.norm_stats['action_std']
                    
                    # Convert to numpy if they're tensors  
                    if torch.is_tensor(action_mean):
                        action_mean = action_mean.cpu().numpy()
                    if torch.is_tensor(action_std):
                        action_std = action_std.cpu().numpy()
                        
                    # Ensure correct shapes and types
                    action_mean = np.array(action_mean, dtype=np.float32)
                    action_std = np.array(action_std, dtype=np.float32)
                    
                    # Ensure action is numpy array
                    action = np.array(action, dtype=np.float32)
                    
                    # Now all operations are numpy -> numpy (no type mismatch)
                    action_denorm = action * action_std + action_mean
                    
                    if self.debug:
                        print(f"‚úÖ Neural network predicted action: {action_denorm}")
                    return action_denorm.astype(np.float32)
                
                # If we get here, something went wrong
                return self._heuristic_action(robot_state)
                
        except Exception as e:
            if self.debug:
                print(f"‚ö†Ô∏è Neural network prediction failed: {e}")
                print("üîÑ Falling back to heuristic action")
            # If prediction fails, gracefully fall back to heuristic
            return self._heuristic_action(robot_state)
    
    def _prepare_inputs(self, image, robot_state, instruction):
        """
        üîß Prepare Model Inputs
        
        This method converts raw inputs (image, robot state, text) into the exact
        format that the neural network expects. Think of it as "translation" between
        human-readable data and machine-readable data.
        
        Steps:
        1. Convert image to PIL format if needed
        2. Normalize robot state using our loaded statistics
        3. Process image through CLIP processor
        4. Format text instruction with special tokens
        5. Tokenize the text into numbers
        6. Package everything into a dictionary
        """
        
        # ======================================================================
        # STEP 1: Process Image
        # ======================================================================
        
        # Convert image to PIL Image if it's a numpy array
        if isinstance(image, np.ndarray):
            # Ensure pixel values are in 0-255 range (standard for images)
            image = Image.fromarray(image.astype(np.uint8))
        
        # ======================================================================
        # STEP 2: Normalize Robot State (FIXED TYPE CONVERSION)
        # ======================================================================
        
        # Ensure robot_state is numpy array
        if torch.is_tensor(robot_state):
            robot_state = robot_state.cpu().numpy()
        robot_state = np.array(robot_state, dtype=np.float32)
        
        # Normalize robot joint positions using loaded statistics
        # Formula: (value - mean) / std_deviation
        # This scales the data to have mean=0, std=1, which neural networks prefer
        if 'state_mean' in self.norm_stats:
            # üî• FIXED: Ensure normalization stats are numpy arrays
            state_mean = self.norm_stats['state_mean']
            state_std = self.norm_stats['state_std']
            
            # Convert to numpy if they're tensors
            if torch.is_tensor(state_mean):
                state_mean = state_mean.cpu().numpy()
            if torch.is_tensor(state_std):
                state_std = state_std.cpu().numpy()
                
            # Ensure correct shapes and types
            state_mean = np.array(state_mean, dtype=np.float32)
            state_std = np.array(state_std, dtype=np.float32)
            
            # Now all operations are numpy -> numpy (no type mismatch)
            norm_state = (robot_state - state_mean) / state_std
            
        elif 'qpos_mean' in self.norm_stats:
            # Fallback to older naming convention with same fixes
            qpos_mean = self.norm_stats['qpos_mean']
            qpos_std = self.norm_stats['qpos_std']
            
            # Convert to numpy if they're tensors
            if torch.is_tensor(qpos_mean):
                qpos_mean = qpos_mean.cpu().numpy()
            if torch.is_tensor(qpos_std):
                qpos_std = qpos_std.cpu().numpy()
                
            # Ensure correct shapes and types
            qpos_mean = np.array(qpos_mean, dtype=np.float32)
            qpos_std = np.array(qpos_std, dtype=np.float32)
            
            # Now all operations are numpy -> numpy
            norm_state = (robot_state - qpos_mean) / qpos_std
            
        else:
            # No normalization stats available
            print("‚ö†Ô∏è No normalization stats available, using raw state")
            norm_state = robot_state
        
        # Convert to PyTorch tensor and add batch dimension (1, 7) instead of (7,)
        state_tensor = torch.from_numpy(norm_state.astype(np.float32)).float().unsqueeze(0).to(self.device)
        
        # ======================================================================
        # STEP 3: Process Image for Model
        # ======================================================================
        
        # Use CLIP image processor to convert PIL image to tensor
        # This handles resizing, normalization, and formatting for the vision model
        image_tensor = self.image_processor(image, return_tensors='pt')['pixel_values'].to(self.device)
        
        # ======================================================================
        # STEP 4: Format Text Instruction
        # ======================================================================
        
        # Create conversation template (how to format text for this model)
        conv = conv_templates['pythia'].copy()
        
        # Format instruction with special image tokens
        # This tells the model: "here's an image, then here's what to do with it"
        inp = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\n' + instruction
        
        # Add instruction to conversation format
        conv.append_message(conv.roles[0], inp)  # Human message
        conv.append_message(conv.roles[1], None) # Assistant message (empty, to be predicted)
        
        # Get the full prompt and add end token
        prompt = conv.get_prompt() + " <|endoftext|>"
        
        # ======================================================================
        # STEP 5: Tokenize Text
        # ======================================================================
        
        # Convert text to token IDs that the model can understand
        # tokenizer_image_token is special - it handles both text and image tokens
        input_ids = tokenizer_image_token(prompt, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(self.device)
        
        # Create attention mask (tells model which tokens to pay attention to)
        attention_mask = torch.ones_like(input_ids)
        
        # ======================================================================
        # STEP 6: Package Everything Together
        # ======================================================================
        
        # Return dictionary with all inputs the model needs
        return {
            'input_ids': input_ids,           # Text as token IDs
            'attention_mask': attention_mask, # Which tokens to attend to
            'images': image_tensor,           # Processed image
            'states': state_tensor,           # Normalized robot state
            'is_pad': torch.zeros(state_tensor.size(0), 16, dtype=torch.bool, device=self.device)  # Required for diffusion head (chunk_size=16)
        }
    
    def _heuristic_action(self, robot_state):
        """
        üé≤ Generate Heuristic Action for Pick-Place Task
        
        This is a "smart fallback" - when the neural network can't predict an action,
        we use this rule-based approach. It's not as smart as the AI, but it's reliable.
        
        Strategy for pick-place task:
        1. If robot end-effector is high up, move down toward object
        2. If robot is not aligned with target, move horizontally to align
        3. If robot is aligned and low, close gripper to grasp
        
        This simulates what a simple robot controller might do.
        
        Args:
            robot_state: Current robot joint positions (first 3 are end-effector x,y,z)
            
        Returns:
            action: [delta_x, delta_y, delta_z, gripper] movement command
        """
        
        # Extract end-effector position from robot state
        # Typically the first 3 values are the x, y, z coordinates of the robot's "hand"
        ee_pos = robot_state[:3]  # End-effector position (x, y, z)
        
        # Define target position in the workspace
        # These are reasonable coordinates for a MetaWorld pick-place task
        target_x, target_y = 0.0, 0.6  # Roughly center of workspace
        
        # Calculate distance to target
        dx = target_x - ee_pos[0]  # How far to move in x direction
        dy = target_y - ee_pos[1]  # How far to move in y direction
        
        # Decide what action to take based on current position
        if ee_pos[2] > 0.05:  # If end-effector is high (z > 5cm)
            # Strategy: Move down toward object, keep gripper open
            action = [dx * 0.1, dy * 0.1, -0.05, -1.0]  # Move down slowly, open gripper
            
        elif abs(dx) > 0.02 or abs(dy) > 0.02:  # If not aligned horizontally (>2cm off)
            # Strategy: Align horizontally, keep gripper open
            action = [dx * 0.2, dy * 0.2, 0.0, -1.0]   # Move horizontally, open gripper
            
        else:  # If aligned and low
            # Strategy: Move down slightly and close gripper to grasp
            action = [0.0, 0.0, -0.02, 1.0]             # Small down movement, close gripper
        
        # Return as numpy array with proper data type
        return np.array(action, dtype=np.float32)


# ==============================================================================
# CONVENIENCE FUNCTIONS - Easy-to-use helper functions
# ==============================================================================

def load_tinyvla(
    lora_checkpoint_path: str,
    base_model_path: str = "lesjie/Llava-Pythia-700M",
    diffusion_weights_path: str = "VLA_weights/diff_head/diffusion_head_latest.bin",
    stats_path: str = "metaworld_stats.pkl",
    device: str = "cuda" if torch.cuda.is_available() else "cpu",
    debug: bool = True
) -> SimpleTinyVLA:
    """
    üöÄ Convenience Function to Load TinyVLA Model
    
    This is a simple function that creates and returns a SimpleTinyVLA instance.
    Use this if you want the simplest possible way to load the model.
    
    Args:
        lora_checkpoint_path: Path to LoRA adapter weights
        base_model_path: Path to base model
        diffusion_weights_path: Path to diffusion head weights
        stats_path: Path to normalization statistics
        device: Device to use ('cuda' or 'cpu')
        debug: Whether to print debug information (False for faster inference)
    
    Example:
        # For development/debugging (verbose output)
        vla = load_tinyvla("VLA_weights/lora_adapter", debug=True)
        
        # For production (silent, faster inference)
        vla = load_tinyvla("VLA_weights/lora_adapter", debug=False)
        
        action = vla.predict_action(image, robot_state, "pick up the block")
    
    Returns:
        SimpleTinyVLA instance ready for making predictions
    """
    return SimpleTinyVLA(base_model_path, lora_checkpoint_path, diffusion_weights_path, stats_path, debug)


def test_loader(debug=True):
    """
    üß™ Test the TinyVLA loader to make sure everything works
    
    This function:
    1. Loads the model
    2. Creates fake test inputs (image + robot state + instruction)
    3. Gets a prediction
    4. Checks that everything worked
    
    Args:
        debug: Whether to use debug mode for detailed output
        
    Returns:
        bool: True if test passes, False if it fails
    """
    
    try:
        print("üß™ Testing TinyVLA Loader...")
        print("üìã Step 1: Loading model...")
        
        # Load model with the working checkpoint path
        vla = load_tinyvla("VLA_weights/full_training_bs1_final/step_1000", debug=debug)
        
        # Step 2: Create dummy test inputs
        print("üìã Step 2: Creating test inputs...")
        dummy_image = Image.new('RGB', (640, 480), 'red')  # Red 640x480 image
        dummy_state = np.zeros(7)  # 7 zeros representing robot joint positions
        
        # Step 3: Get a prediction
        print("üìã Step 3: Getting prediction...")
        action = vla.predict_action(dummy_image, dummy_state)
        
        # Step 4: Report success
        print(f"‚úÖ Test successful! Action: {action}")
        print("üéâ TinyVLA loader is working correctly!")
        
        return True
        
    except Exception as e:
        # If anything goes wrong, print detailed error information
        print(f"‚ùå Test failed: {e}")
        import traceback
        traceback.print_exc()
        print("üí° Check that all model files are in the correct locations")
        
        return False


# ==============================================================================
# MAIN EXECUTION - Run test if this script is executed directly
# ==============================================================================

if __name__ == "__main__":
    # If someone runs this script directly (python tinyvla_loader.py),
    # run the test function to verify everything works
    test_loader() 