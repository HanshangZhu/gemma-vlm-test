import torch
import torch.nn as nn
from transformers import AutoProcessor, GemmaForCausalLM, GemmaTokenizer
from transformers.models.siglip.modeling_siglip import SiglipVisionModel
from torchvision import transforms
from PIL import Image

class CustomVLM(nn.Module):
    def __init__(self, 
                 image_encoder_ckpt="google/siglip-base-patch16-224",
                 proj_ckpt_path="siglip_proj_head.pt",
                 decoder_ckpt="google/gemma-3-1b-it"):
        super().__init__()

        # Load pretrained SigLIP encoder
        self.image_encoder = SiglipVisionModel.from_pretrained(image_encoder_ckpt)
        self.image_encoder.eval()

        # Projection head: 768 -> 2048
        self.proj = nn.Linear(768, 2048)
        self.proj.load_state_dict(torch.load(proj_ckpt_path))
        self.proj.eval()

        # Load Gemma decoder (language model)
        self.decoder = GemmaForCausalLM.from_pretrained(decoder_ckpt)
        self.tokenizer = GemmaTokenizer.from_pretrained(decoder_ckpt)

        # Define image preprocessor to match SigLIP input
        self.preprocess = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
        ])

    def forward(self, image_tensor, prompt_text, device="cuda"):
        """
        image_tensor: preprocessed tensor [1, 3, 224, 224]
        prompt_text: str
        """
        # Step 1: Encode image -> patch embedding -> project
        with torch.no_grad():
            vision_outputs = self.image_encoder(pixel_values=image_tensor.to(device))
            vision_embed = vision_outputs.last_hidden_state[:, 0, :]  # [CLS] token
            projected = self.proj(vision_embed)

        # Step 2: Tokenize text prompt
        inputs = self.tokenizer(prompt_text, return_tensors="pt").to(device)

        # Step 3: Naively add projected image embedding to token embeddings
        projected = projected.unsqueeze(1).repeat(1, inputs.input_ids.size(1), 1)
        inputs_embeds = self.decoder.get_input_embeddings()(inputs.input_ids)
        fused_inputs = inputs_embeds + projected

        # Step 4: Generate
        with torch.no_grad():
            outputs = self.decoder.generate(inputs_embeds=fused_inputs, max_new_tokens=64)
        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)

    def infer_from_path(self, image_path, prompt_text, device="cuda"):
        image = Image.open(image_path).convert("RGB")
        image_tensor = self.preprocess(image).unsqueeze(0)
        return self.forward(image_tensor, prompt_text, device=device)


if __name__ == "__main__":
    model = CustomVLM()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    image_path = "example.jpg"  # Replace with actual image
    prompt = "Describe the image."
    caption = model.infer_from_path(image_path, prompt, device=device)
    print("Generated Caption:", caption)
