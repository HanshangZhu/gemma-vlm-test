# === TinyVLA LoRA training on full MT50 (HPC) =============================
# Name: train_hpc_700m_mt50.yaml
# Purpose: run on 4×40 GB A100 using BF16, frozen vision tower.
# -------------------------------------------------------------------------
# Model / paths
model_path: lesjie/Llava-Pythia-700M   # 700 M backbone; auto-download once
output_dir: outputs/mt50_hpc_run       # will get timestamp suffix if exists

# Dataset -----------------------------------------------------------------
dataset_variant: mt50                  # full HuggingFace MT50 dataset
prompt_json_path: datasets/mt50_task_prompts.json
prompt_style: mixed                    # enables 40/30/30 simple/detailed/very
validate_samples_count: 0              # skip per-sample validation for speed

# LoRA hyper-params --------------------------------------------------------
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05

# Training schedule --------------------------------------------------------
batch_size: 24                   # PER GPU (4 GPUs = 96 per step)
gradient_accumulation_steps: 2    # → effective batch 192 (4×24×2)
learning_rate: 5.0e-6             # Scaled up for 4 GPUs and larger batch
diffusion_learning_rate: 1.2e-6   # Scaled proportionally
max_steps: 28800                  # 12 epochs over 205k frames @ eff-BS 192
save_steps: 2400                  # Save every epoch (28800/12 = 2400 steps)

# Diffusion head -----------------------------------------------------------
train_diffusion_head: true
diffusion_warmup_steps: 0

diffusion_head_save_dir: outputs/mt50_hpc_run/diff_head

# Memory / precision -------------------------------------------------------
use_bf16: true
use_gradient_checkpointing: true
freeze_vision_encoder: true        # keep CLIP frozen to save memory
cpu_offload: false
max_memory_cleanup_steps: 500       # empty CUDA cache every 500 steps
chunk_size: 8
image_size: 336                     # 336×336 CLIP input

# Dataloader ---------------------------------------------------------------
dataloader_num_workers: 8          # Increased for 4 GPUs
pin_memory: true
persistent_workers: true

# Optimisation & stability -------------------------------------------------
weight_decay: 0.01
gradient_clip_norm: 0.3
warmup_steps: 2000                  # 2 k-step LR warm-up

# Misc ---------------------------------------------------------------------
max_memory_cleanup_steps: 500

# Distributed Training -----------------------------------------------------
deepspeed_config_path: "ds_config_stage2.json" # Path to DeepSpeed config file
# Set to null or remove this line to fall back to single-GPU training 