model_path: VLA_weights/Llava-Pythia-400M  # keep local to avoid download
output_dir: mt50_memopt_run

# Dataset settings
dataset_variant: mt50_dummy  # use lightweight synthetic data
dummy_mt50_samples: 200
prompt_style: mixed  # triggers 40/30/30 random prompt sampling
prompt_json_path: datasets/mt50_task_prompts.json
validate_samples_count: 0  # skip validation for speed

# LoRA parameters
lora_r: 4
lora_alpha: 8
lora_dropout: 0.1

# Training hyper-params
effective_batch_size: 4  # 1 Ã— grad_accum 4
batch_size: 1
gradient_accumulation_steps: 4
learning_rate: 1e-6
diffusion_learning_rate: 5e-7
max_steps: 20
save_steps: 20
train_diffusion_head: true

diffusion_warmup_steps: 0

# Memory optimisation
use_bf16: true
use_gradient_checkpointing: true
freeze_vision_encoder: true  # vision tower weights stay frozen
cpu_offload: false
max_memory_cleanup_steps: 10
chunk_size: 8
image_size: 224  # smaller resolution lowers VRAM

dataloader_num_workers: 0
pin_memory: false
persistent_workers: false

gradient_clip_norm: 0.3
warmup_steps: 5
weight_decay: 0.01

diffusion_head_save_dir: mt50_memopt_run/diff_head 